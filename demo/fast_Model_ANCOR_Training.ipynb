{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building ANCOR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from french_crs.ancor2dataset import dataset_builder\n",
    "\n",
    "# Do-able with two nested loops\n",
    "# for method in [\"balanced\", \"representative\", \"window\"]:\n",
    "#     for subcorpus in [[\"INDIRECTE\"],[\"DIRECTE\"],[\"ANAPHORE\"]]:\n",
    "\n",
    "dataset_params={\n",
    "    # There are other three possible strategies: (1)\"balanced\", (2)\"representative\" and (3)\"window\"\n",
    "    \"strategy\" : \"balanced,\n",
    "    \"ancor_corpus_path\" : \"../DISTRIB_ANCOR/\",\n",
    "    # All possibilities are: [\"corpus_OTG\",\"corpus_UBS\",\"corpus_ESLO\" ,\"corpus_ESLO_CO2\"]\n",
    "    \"sub_corpus_filter\" : [\"corpus_OTG\",\"corpus_UBS\",\"corpus_ESLO\" ,\"corpus_ESLO_CO2\"], \n",
    "    # All possibilities are: [\"DIRECTE\", \"INDIRECTE\", \"ANAPHORE\"]\n",
    "    \"coreference_type_filter\" : [\"DIRECTE\"], \n",
    "    \"dataset_output_folder\" : \"../datasets/\",\n",
    "    # This will be used only in window strategy\n",
    "    \"window_size\" : 5 \n",
    "}\n",
    "\n",
    "dataset = dataset_builder(**dataset_params)\n",
    "dataset.build_dataset(file_analysis_alert=True)\n",
    "dataset.merge_dataset(delete_original_after_merge=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_file_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8e6b6efe4bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msubfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mroot_xml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroot_aa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_file_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mjson_mentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_json_mentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_xml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroot_aa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mjson_mentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_empty_json_mentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_mentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_file_config' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "json_relations_lined_up={}\n",
    "counter=0\n",
    "\n",
    "for sub_corpus in [\"corpus_OTG\",\"corpus_UBS\",\"corpus_ESLO\",\"corpus_ESLO_CO2\"]:\n",
    "\n",
    "    ancor_subcorpus_path = \"../DISTRIB_ANCOR/\"+sub_corpus+'/'\n",
    "\n",
    "    corpus_files = [file[file.rfind(\n",
    "        \"/\")+1:-4] for file in glob.glob(ancor_subcorpus_path+\"annotation_integree/*.xml\")]\n",
    "\n",
    "    list_dataframes = []\n",
    "\n",
    "    for subfile in corpus_files:\n",
    "\n",
    "        root_xml,root_aa,data_source=set_file_config(sub_corpus, subfile)\n",
    "        json_mentions=generate_json_mentions(root_xml,root_aa)\n",
    "        json_mentions=remove_empty_json_mentions(json_mentions)\n",
    "        json_relations=generate_json_relations()\n",
    "        if json_relations == {}:\n",
    "            continue\n",
    "        json_coreference_chains=generate_json_chains()\n",
    "        \n",
    "        print(\"starting\",subfile,\"at\",sub_corpus)\n",
    "        \n",
    "        for pair_id in json_relations:            \n",
    "\n",
    "            if json_relations[pair_id][\"TYPE\"] in [\"DIRECTE\", \"INDIRECTE\", \"ANAPHORE\"]:\n",
    "                \n",
    "                counter+=1\n",
    "                coref_features=generate_coref_relations_json_features(pair_id,counter,json_relations,\n",
    "                                                                      json_mentions,data_source,\n",
    "                                                                      is_positive_instance=True)\n",
    "                json_relations_lined_up[counter]=coref_features\n",
    "\n",
    "            \n",
    "                list_mentions_sorted_seperated=[]\n",
    "                df_mentions = pd.DataFrame.from_dict(json_mentions, orient='index')\n",
    "                df_mentions = df_mentions.sort_values(by=['START_ID'])\n",
    "                list_mentions_sorted = list(df_mentions.index)\n",
    "                \n",
    "                relation_left=json_relations[pair_id][\"LEFT_UNIT\"][\"ID\"]\n",
    "                relation_right=json_relations[pair_id][\"RIGHT_UNIT\"][\"ID\"]\n",
    "                \n",
    "                if json_mentions[relation_left][\"START_ID\"] > json_mentions[relation_right][\"START_ID\"]:\n",
    "                    relation_left,relation_right=relation_right,relation_left\n",
    "\n",
    "                start_index=list_mentions_sorted.index(relation_left)+1\n",
    "                end_index=list_mentions_sorted.index(relation_right)\n",
    "\n",
    "                mentions_in_between=list_mentions_sorted[start_index:end_index]\n",
    "\n",
    "                result=\"Nothing\"\n",
    "                for ment in reversed(mentions_in_between):\n",
    "                    for index, chain in json_coreference_chains.items():\n",
    "                        if not (ment in json_coreference_chains[index] and  relation_right in json_coreference_chains[index]): \n",
    "                                result=(ment,relation_right)\n",
    "                                break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    break\n",
    "                    \n",
    "                if result!=\"Nothing\":\n",
    "                    \n",
    "                    counter+=1\n",
    "                    coref_features=generate_coref_relations_json_features(-1,counter,json_relations,\n",
    "                                                                          json_mentions,data_source,\n",
    "                                                                          is_positive_instance=False,\n",
    "                                                                          pair_left_id=result[0],\n",
    "                                                                          pair_right_id=result[1])\n",
    "                    json_relations_lined_up[counter]=coref_features\n",
    "                    \n",
    "                    \n",
    "\n",
    "            if json_relations[pair_id][\"TYPE\"] in [\"ASSOC\", \"ASSOC_PRONOM\"]:\n",
    "\n",
    "                counter+=1\n",
    "                coref_features=generate_coref_relations_json_features(pair_id,counter,json_relations,\n",
    "                                                                      json_mentions,data_source,\n",
    "                                                                      is_positive_instance=True,\n",
    "                                                                      is_assoc=True)\n",
    "                json_relations_lined_up[counter]=coref_features\n",
    "\n",
    "\n",
    "df_coref_relations = pd.DataFrame.from_dict(json_relations_lined_up, orient='index')\n",
    "df_coref_relations.to_excel(\"Dataset_ANCOR_Balanced.xlsx\")\n",
    "\n",
    "df_coref_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coref_relations_json_features(pair_id,counter_id,json_relations,json_mentions,\n",
    "                                           data_source,is_positive_instance=True,\n",
    "                                           pair_left_id=None,pair_right_id=None,is_assoc=False):\n",
    "\n",
    "    if is_positive_instance:\n",
    "        relation_type=json_relations[pair_id][\"TYPE\"]\n",
    "        relation_left=json_relations[pair_id][\"LEFT_UNIT\"]\n",
    "        relation_right=json_relations[pair_id][\"RIGHT_UNIT\"]\n",
    "        \n",
    "        if relation_left[\"START_ID\"] > relation_right[\"START_ID\"]:\n",
    "            relation_left,relation_right=relation_right,relation_left\n",
    "        \n",
    "        MENTION_LEFT_ID=relation_left[\"ID\"]\n",
    "        MENTION_RIGHT_ID=relation_right[\"ID\"]\n",
    "        \n",
    "        if is_assoc:\n",
    "            is_positive_instance=False\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        relation_type=\"NON_COREFERENCE\"\n",
    "        relation_left=json_mentions[pair_left_id]\n",
    "        relation_right=json_mentions[pair_right_id]\n",
    "        \n",
    "        if relation_left[\"START_ID\"] > relation_right[\"START_ID\"]:\n",
    "            relation_left,relation_right=relation_right,relation_left\n",
    "        \n",
    "        MENTION_LEFT_ID=pair_left_id\n",
    "        MENTION_RIGHT_ID=pair_right_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    string_text = data_source[relation_left[\"END_ID\"]:relation_right[\"START_ID\"]]\n",
    "    string_text=re.sub(\"<[a-zA-Z0-9\\s=\\\",/.]+>\", \" \", string_text).strip().split()\n",
    "    DISTANCE_MENTION = int(relation_right[\"NUM\"])-int(relation_left[\"NUM\"])-1\n",
    "    DISTANCE_WORD = len(string_text)\n",
    "    DISTANCE_CHAR = len(\"\".join(string_text))\n",
    "\n",
    "\n",
    "    tf2yn = {True: \"YES\", False: \"NO\"}\n",
    "\n",
    "    left_content = str(relation_left[\"CONTENT\"])\n",
    "    right_content = str(relation_right[\"CONTENT\"])\n",
    "    left_content_list = left_content.lower().split()\n",
    "    right_content_list = right_content.lower().split()\n",
    "    len_of_words_min = min(len(left_content_list), len(right_content_list))\n",
    "    len_of_words_max = max(len(left_content_list), len(right_content_list))\n",
    "    len_intersection = len(\n",
    "        [value for value in left_content_list if value in right_content_list])\n",
    "\n",
    "\n",
    "    ID_FORM = tf2yn[relation_left[\"CONTENT\"]==relation_right[\"CONTENT\"]]\n",
    "    ID_SUBFORM = tf2yn[len_intersection>0]\n",
    "\n",
    "    INCL_RATE = len_intersection/len_of_words_min\n",
    "    COM_RATE = len_intersection/len_of_words_max\n",
    "\n",
    "    ID_DEF = tf2yn[relation_left[\"DEF\"]==relation_right[\"DEF\"]]\n",
    "    if relation_left[\"DEF\"] == \"UNK\" or relation_right[\"DEF\"] == \"UNK\":\n",
    "        ID_DEF = \"UNK\"\n",
    "\n",
    "    ID_GP = tf2yn[relation_left[\"GP\"]==relation_right[\"GP\"]]\n",
    "    if relation_left[\"GP\"] == \"UNK\" or relation_right[\"GP\"] == \"UNK\":\n",
    "        ID_GP = \"UNK\"\n",
    "\n",
    "    ID_TYPE = tf2yn[relation_left[\"TYPE\"]==relation_right[\"TYPE\"]]\n",
    "    if relation_left[\"TYPE\"] == \"UNK\" or relation_right[\"TYPE\"] == \"UNK\":\n",
    "        ID_TYPE = \"UNK\"\n",
    "\n",
    "    ID_EN = tf2yn[relation_left[\"EN\"]==relation_right[\"EN\"]]\n",
    "    if relation_left[\"EN\"] == \"UNK\" or relation_right[\"EN\"] == \"UNK\":\n",
    "        ID_EN = \"UNK\"\n",
    "\n",
    "    ID_GENDER = tf2yn[relation_left[\"GENRE\"]==relation_right[\"GENRE\"]]\n",
    "    if relation_left[\"GENRE\"] == \"UNK\" or relation_right[\"GENRE\"] == \"UNK\":\n",
    "        ID_GENDER = \"UNK\"\n",
    "\n",
    "    ID_NUMBER = tf2yn[relation_left[\"NUM\"]==relation_right[\"NUM\"]]\n",
    "    if relation_left[\"NUM\"] == \"UNK\" or relation_right[\"NUM\"] == \"UNK\":\n",
    "        ID_NUMBER = \"UNK\"\n",
    "\n",
    "    EMBEDDED = tf2yn[((left_content in right_content) or (right_content in left_content))]\n",
    "\n",
    "    ID_PREVIOUS = tf2yn[relation_left[\"PREVIOUS\"]==relation_right[\"PREVIOUS\"]]\n",
    "    if relation_left[\"PREVIOUS\"] == \"^\" or relation_right[\"PREVIOUS\"] == \"^\":\n",
    "        ID_PREVIOUS = \"UNK\"\n",
    "\n",
    "    ID_NEXT = tf2yn[relation_left[\"NEXT\"]==relation_right[\"NEXT\"]]\n",
    "    if relation_left[\"NEXT\"] == \"^\" or relation_right[\"NEXT\"] == \"^\":\n",
    "        ID_NEXT = \"UNK\"\n",
    "        \n",
    "    coref_relation_json_feature={\n",
    "        \n",
    "                                \"SUB_CORPUS\":sub_corpus,\n",
    "                                \"FILE_NAME\":subfile,\n",
    "                                \"COREF_TABLE_ID\":counter_id,\n",
    "                                \"COREF_TYPE\":relation_type,\n",
    "                                \"COREF_ANCOR_ID\":pair_id,\n",
    "                                \"MENTION_LEFT_ID\":MENTION_LEFT_ID,\n",
    "                                \"MENTION_RIGHT_ID\":MENTION_RIGHT_ID,\n",
    "                                \"LEFT_CONTENT\":relation_left[\"CONTENT\"],                                     \n",
    "                                \"RIGHT_CONTENT\":relation_right[\"CONTENT\"],                                     \n",
    "                                \"LEFT_TYPE\":relation_left[\"TYPE\"], \n",
    "                                \"RIGHT_TYPE\":relation_right[\"TYPE\"], \n",
    "                                \"LEFT_DEF\":relation_left[\"DEF\"],\n",
    "                                \"RIGHT_DEF\":relation_right[\"DEF\"], \n",
    "                                \"LEFT_GP\":relation_left[\"GP\"],\n",
    "                                \"RIGHT_GP\":relation_right[\"GP\"],\n",
    "                                \"LEFT_GENRE\":relation_left[\"GENRE\"],\n",
    "                                \"RIGHT_GENRE\":relation_right[\"GENRE\"],\n",
    "                                \"LEFT_NB\":relation_left[\"NB\"],\n",
    "                                \"RIGHT_NB\":relation_right[\"NB\"],\n",
    "                                \"LEFT_EN\":relation_left[\"EN\"],\n",
    "                                \"RIGHT_EN\":relation_right[\"EN\"],\n",
    "                                \"ID_FORM\": ID_FORM, \n",
    "                                \"ID_SUBFORM\": ID_SUBFORM, \n",
    "                                \"INCL_RATE\": INCL_RATE, \n",
    "                                \"COM_RATE\": COM_RATE, \n",
    "                                \"ID_DEF\": ID_DEF, \n",
    "                                \"ID_GP\": ID_GP, \n",
    "                                \"ID_TYPE\": ID_TYPE,\n",
    "                                \"ID_EN\": ID_EN, \n",
    "                                \"ID_GENDER\": ID_GENDER, \n",
    "                                \"ID_NUMBER\": ID_NUMBER, \n",
    "                                \"DISTANCE_MENTION\": DISTANCE_MENTION, \n",
    "                                \"DISTANCE_WORD\": DISTANCE_WORD,\n",
    "                                \"DISTANCE_CHAR\": DISTANCE_CHAR, \n",
    "                                \"EMBEDDED\": EMBEDDED, \n",
    "                                \"ID_PREVIOUS\": ID_PREVIOUS, \n",
    "                                \"ID_NEXT\": ID_NEXT, \n",
    "                                \"IS_CO_REF\": int(is_positive_instance)\n",
    "    }\n",
    "    \n",
    "    return(coref_relation_json_feature)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "\n",
    "def set_file_config(sub_corpus, file):\n",
    "\n",
    "    path_file = \"../DISTRIB_ANCOR/\"+sub_corpus+'/'\n",
    "\n",
    "    tree_xml = ET.parse(\n",
    "        path_file+'annotation_integree/'+file+'.xml')\n",
    "    tree_aa = ET.parse(\n",
    "        path_file+'aa_fichiers/'+file+'.aa')\n",
    "    data_source = open(\n",
    "        path_file+'ac_fichiers/'+file + '.ac', \"r\").read()\n",
    "\n",
    "    root_xml = tree_xml.getroot()\n",
    "    root_aa = tree_aa.getroot()\n",
    "    \n",
    "    return(root_xml,root_aa,data_source)\n",
    "    \n",
    "\n",
    "# return mentions in json format\n",
    "def generate_json_mentions(root_xml,root_aa):\n",
    "\n",
    "    json_mentions = {}\n",
    "\n",
    "    for unit in root_xml.iter(\"unit\"):\n",
    "        data_character = {}\n",
    "        data_character[\"TYPE\"] = unit.find(\"./characterisation/type\").text\n",
    "\n",
    "        for anchor in root_xml.iter(\"anchor\"):\n",
    "            if anchor.attrib[\"id\"] == unit.attrib[\"id\"]:\n",
    "                data_character[\"NUM\"] = anchor.attrib[\"num\"]\n",
    "\n",
    "        for feat in unit.findall(\"./characterisation/featureSet/feature\"):\n",
    "            data_character[feat.attrib[\"name\"]] = feat.text\n",
    "\n",
    "        for unit_aa in root_aa.iter(\"unit\"):\n",
    "            if unit_aa.attrib[\"id\"] == unit.attrib[\"id\"]:\n",
    "                data_character[\"START_ID\"] = int(unit_aa.find(\n",
    "                    \"./positioning/start/singlePosition\").attrib[\"index\"])\n",
    "                data_character[\"END_ID\"] = int(unit_aa.find(\n",
    "                    \"./positioning/end/singlePosition\").attrib[\"index\"])\n",
    "\n",
    "        json_mentions[unit.attrib[\"id\"]] = data_character\n",
    "\n",
    "    return(json_mentions)\n",
    "\n",
    "# return mentions in json format and eliminates all the features that are not presented.\n",
    "def remove_empty_json_mentions(json_mentions,features_to_check=[\"CONTENT\", \"PREVIOUS\"]):\n",
    "\n",
    "    json_mentions_= json_mentions.copy()\n",
    "    for mention in list(json_mentions_):\n",
    "        for feature in features_to_check:\n",
    "            if feature not in json_mentions_[mention]:\n",
    "                del json_mentions_[mention]\n",
    "                break\n",
    "\n",
    "    return(json_mentions_)\n",
    "\n",
    "\n",
    "# return relations in json format based on the json mentions\n",
    "def generate_json_relations():\n",
    "\n",
    "    json_relations={}\n",
    "\n",
    "    for relation in root_aa.iter(\"relation\"):\n",
    "        data_character = {}\n",
    "        data_character[\"TYPE\"] = relation.find(\"./characterisation/type\").text\n",
    "\n",
    "        for feat in relation.findall(\"./characterisation/featureSet/feature\"):\n",
    "            data_character[feat.attrib[\"name\"]] = feat.text\n",
    "\n",
    "        unit_ids = []\n",
    "        for feat in relation.findall(\"./positioning/term\"):\n",
    "            unit_ids.append(feat.attrib[\"id\"])\n",
    "\n",
    "        if (unit_ids[0] in json_mentions.keys()) and (unit_ids[1] in json_mentions.keys()):\n",
    "            if (len(unit_ids) == 2):\n",
    "                data_character[\"LEFT_UNIT\"] = {\n",
    "                    \"ID\": unit_ids[0], **json_mentions[unit_ids[0]]}\n",
    "                data_character[\"RIGHT_UNIT\"] = {\n",
    "                    \"ID\": unit_ids[1], **json_mentions[unit_ids[1]]}\n",
    "\n",
    "            json_relations[relation.attrib[\"id\"]] = data_character\n",
    "\n",
    "    return(json_relations)\n",
    "\n",
    "\n",
    "# return chains in json format based on the json mentions and relations.\n",
    "def generate_json_chains(coreference_type=[\"DIRECTE\", \"INDIRECTE\", \"ANAPHORE\"]):\n",
    "    G = nx.Graph()\n",
    "    chain_id = 0\n",
    "    json_coreference_chains = {}\n",
    "\n",
    "    for chain in json_relations:\n",
    "        if json_relations[chain][\"TYPE\"].strip() in coreference_type:\n",
    "            chain_left = json_relations[chain][\"LEFT_UNIT\"][\"ID\"]\n",
    "            chain_right = json_relations[chain][\"RIGHT_UNIT\"][\"ID\"]\n",
    "            G.add_edge(chain_left, chain_right,\n",
    "                       coref_type=json_relations[chain][\"TYPE\"].strip())\n",
    "\n",
    "    for subgraph in list(nx.connected_components(G)):\n",
    "        json_coreference_chains[chain_id] = []\n",
    "        for node in subgraph:\n",
    "            json_coreference_chains[chain_id].append(node)\n",
    "        chain_id += 1\n",
    "\n",
    "    return(json_coreference_chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two ways of Training: (1) commented codes down with parameter.\n",
    "#                       (2) uncommented with mentioned explicity the config file.\n",
    "\n",
    "from french_crs.fast_model_training import dataset_splitter\n",
    "\n",
    "# ds1=dataset_splitter(\"./Dataset_ANCOR_Balanced.xlsx\",\n",
    "#                     \"./Dataset_ANCOR_Balanced_Train.xlsx\",\n",
    "#                     \"./Dataset_ANCOR_Balanced_Test.xlsx\",\n",
    "#                       split_config_json=\"./split_config.json\")\n",
    "\n",
    "# ds1.dataset_splitter_by_file(lower_rate=0.20,upper_rate=0.50, files_num=68)\n",
    "\n",
    "\n",
    "ds2=dataset_splitter(\"./Dataset_ANCOR_Balanced.xlsx\",\n",
    "                    \"./Dataset_ANCOR_Balanced_Train.xlsx\",\n",
    "                    \"./Dataset_ANCOR_Balanced_Test.xlsx\",\n",
    "                      split_config_json=\"./split_config.json\")\n",
    "\n",
    "dict_files=ds2.dataset_splitter_by_json_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Name : Model_ANCOR_balanced_Full_Soon.model\n",
      "\n",
      "Train Dataset : balanced_Full_Soon_Train.xlsx\n",
      "Test  Dataset : balanced_Full_Soon_Test.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision_score_pos': 0.8380271822894774,\n",
       " 'recall_score_pos': 0.8590922148807814,\n",
       " 'f1_score_pos': 0.8484289665933754,\n",
       " 'precision_score_neg': 0.8545373665480427,\n",
       " 'recall_score_neg': 0.8329238329238329,\n",
       " 'f1_score_neg': 0.8435921832686818}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from french_crs.fast_model_training import model_trainer\n",
    "\n",
    "\"\"\"\n",
    "12 possibilities for \"train\" and \"test\" variables\n",
    "\n",
    "balanced_ANAPHORE\n",
    "balanced_DIRECTE\n",
    "balanced_Full\n",
    "balanced_INDIRECTE\n",
    "representative_ANAPHORE\n",
    "representative_DIRECTE\n",
    "representative_Full\n",
    "representative_INDIRECTE\n",
    "window_ANAPHORE\n",
    "window_DIRECTE\n",
    "window_Full\n",
    "window_INDIRECTE\n",
    "\"\"\"\n",
    "\n",
    "train=\"balanced_Full\"\n",
    "test=\"balanced_Full_Soon\"\n",
    "\n",
    "model=model_trainer( \"../datasets/\"+ train+\"/\"+train+\"_Train.xlsx\",\n",
    "                     \"../datasets/\"+ test+\"/\"+test+\"_Test.xlsx\",\n",
    "                     \"../datasets/\"+ train+\"/\"+train+\"_Test_Pred.xlsx\",\n",
    "                     \"IS_CO_REF\",\n",
    "                     \"IS_CO_REF\"\n",
    "                   )\n",
    "\n",
    "model.columns_drop_list = [\"SUB_CORPUS\",\n",
    "                            \"FILE_NAME\",\n",
    "                            \"COREF_TABLE_ID\",\n",
    "                            \"COREF_TYPE\",\n",
    "                            \"COREF_ANCOR_ID\",\n",
    "                            \"MENTION_LEFT_ID\",\n",
    "                            \"MENTION_RIGHT_ID\",\n",
    "                            \"LEFT_CONTENT\",\n",
    "                            \"RIGHT_CONTENT\",\n",
    "                            \"LEFT_DEF\",\n",
    "                            \"RIGHT_DEF\",\n",
    "                            \"ID_DEF\",\n",
    "                            \"DISTANCE_MENTION\",\n",
    "                            \"DISTANCE_WORD\",\n",
    "                            \"DISTANCE_CHAR\"]\n",
    "\n",
    "model.convert_columns_to_numeric()\n",
    "print(\"Model_Name :\",\"Model_ANCOR_\"+train+\".model\\n\")\n",
    "print(\"Train Dataset :\",train+\"_Train.xlsx\")\n",
    "print(\"Test  Dataset :\",test+\"_Test.xlsx\")\n",
    "performance=model.train_model_random_forest(model_name=\"../pre-trained language models/Model_ANCOR_\"+train+\".model\",max_depth=10, random_state=0,n_estimators=250)\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=\"balanced_Full\"\n",
    "test=\"window_Full\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCORCH Chains Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORCH json formats saved at following addresses:\n",
      "/Users/mehdi.mirzapour/French-CRS/coref_chains_gold.json\n",
      "/Users/mehdi.mirzapour/French-CRS/coref_chains_pred.json\n"
     ]
    }
   ],
   "source": [
    "from french_crs.pairs2chains import chains_builder\n",
    "\n",
    "model_chains=chains_builder(path_gold_file=\"../datasets/\"+ train+\"/\"+train+\"_Test_Pred.xlsx\", \n",
    "                            path_model_file=\"../datasets/\"+ train+\"/\"+train+\"_Test_Pred.xlsx\",\n",
    "                            gold_column=\"IS_CO_REF\",\n",
    "                            model_column=\"Prediction\",\n",
    "                            scorch_output_path=\"../\",\n",
    "                            threshold=0.5)\n",
    "\n",
    "model_chains.generate_gold_model_json_output(mode=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCORCH Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "bashCommand = \"scorch ../coref_chains_gold_b.json ../coref_chains_pred_r.json > ../mm.txt\"\n",
    "os.system(bashCommand)\n",
    "f = open(\"../mm.txt\",'r')\n",
    "message = f.read()\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting All Chains Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import os\n",
    "from french_crs.model_training import model_trainer\n",
    "from french_crs.pairs2chains import chains_builder\n",
    "\n",
    "mydoc = docx.Document()\n",
    "\n",
    "style = mydoc.styles['Normal']\n",
    "font = style.font\n",
    "font.name = 'MS Gothic'\n",
    "font.size = docx.shared.Pt(10)\n",
    "\n",
    "\n",
    "train_test_list=[\n",
    "    \"balanced_ANAPHORE\",\n",
    "    \"balanced_DIRECTE\",\n",
    "    \"balanced_Full\",\n",
    "    \"balanced_INDIRECTE\",\n",
    "    \"representative_ANAPHORE\",\n",
    "    \"representative_DIRECTE\",\n",
    "    \"representative_Full\",\n",
    "    \"representative_INDIRECTE\",\n",
    "    \"window_ANAPHORE\",\n",
    "    \"window_DIRECTE\",\n",
    "    \"window_Full\",\n",
    "    \"window_INDIRECTE\"\n",
    "    ]\n",
    "\n",
    "# train_test_list=[\n",
    "#     [\"balanced_ANAPHORE\",\"window_ANAPHORE\"],\n",
    "#     [\"balanced_DIRECTE\",\"window_DIRECTE\"],\n",
    "#     [\"balanced_Full\",\"window_Full\"],\n",
    "#     [\"balanced_INDIRECTE\",\"window_INDIRECTE\"],\n",
    "#     [\"representative_ANAPHORE\",\"window_ANAPHORE\"],\n",
    "#     [\"representative_DIRECTE\",\"window_DIRECTE\"],\n",
    "#     [\"representative_Full\",\"window_Full\"],\n",
    "#     [\"representative_INDIRECTE\",\"window_INDIRECTE\"],\n",
    "#     [\"window_ANAPHORE\",\"window_ANAPHORE\"],\n",
    "#     [\"window_DIRECTE\",\"window_DIRECTE\"],\n",
    "#     [\"window_Full\",\"window_Full\"],\n",
    "#     [\"window_INDIRECTE\",\"window_INDIRECTE\"]\n",
    "#     ]\n",
    "\n",
    "counter=0\n",
    "\n",
    "# for train in train_test_list:\n",
    "#     for test in train_test_list:\n",
    "\n",
    "for train in train_test_list:\n",
    "    \n",
    "    test=train\n",
    "    \n",
    "    counter+=1\n",
    "\n",
    "    model=model_trainer( \"../datasets/\"+ train+\"/\"+train+\"_Train.xlsx\",\n",
    "                 \"../datasets/\"+ test+\"/\"+test+\"_Test.xlsx\",\n",
    "                 \"../datasets/\"+ train+\"/\"+train+\"_Test_Pred.xlsx\",\n",
    "                 \"IS_CO_REF\",\n",
    "                 \"IS_CO_REF\"\n",
    "               )\n",
    "\n",
    "    model.columns_drop_list = ['m1_DEF', 'm2_DEF', 'ID_DEF',\n",
    "                               'DISTANCE_MENTION','DISTANCE_WORD',\n",
    "                               'DISTANCE_CHAR']\n",
    "\n",
    "    model.convert_columns_to_numeric()\n",
    "    performance=model.train_model_random_forest(model_name=\"../pre-trained language models/Model_ANCOR_\"+train+\".model\",max_depth=10, random_state=0,n_estimators=250)\n",
    "\n",
    "\n",
    "    model_chains=chains_builder(path_gold_file=\"../datasets/\"+ train+\"/\"+train+\"_Test_Pred.xlsx\", \n",
    "                        path_model_file=\"../datasets/\"+ train+\"/\"+train+\"_Test_Pred.xlsx\",\n",
    "                        gold_column=\"IS_CO_REF\",\n",
    "                        model_column=\"Prediction\",\n",
    "                        scorch_output_path=\"../\",\n",
    "                        threshold=0.5)\n",
    "\n",
    "    model_chains.generate_gold_model_json_output(mode=\"train\")\n",
    "\n",
    "\n",
    "    bashCommand = \"scorch ../coref_chains_gold.json ../coref_chains_pred.json > ../output.txt\"\n",
    "    os.system(bashCommand)\n",
    "    f = open(\"../output.txt\",'r')\n",
    "    message = f.read()\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    mydoc.add_paragraph(\"Model_Name : \"+\"Model_ANCOR_\"+train+\".model\")\n",
    "    mydoc.add_paragraph(\"Train Dataset : \"+train+\"_Train.xlsx\")\n",
    "    mydoc.add_paragraph(\"Test  Dataset : \"+test+\"_Test.xlsx\")\n",
    "    mydoc.add_paragraph(\"\\n\")\n",
    "    mydoc.add_paragraph(str(performance))\n",
    "    mydoc.add_paragraph(\"\\n\")\n",
    "    mydoc.add_paragraph(message)\n",
    "    mydoc.add_page_break()\n",
    "\n",
    "    print(counter)\n",
    "\n",
    "\n",
    "mydoc.save(\"../pre-trained language models/Performance Analysis.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_training import model_tester\n",
    "\n",
    "model_parameter={\n",
    "                \"model_name\" : \"./Models/Random_Forest_(Normal)_OTG_Neg_90_Pos_10.model\",\n",
    "                \"input_file\" : \"./Datasets/corpus_ALL_Window_30_Test.xlsx\",\n",
    "                \"output_file\" : \"./Datasets/corpus_ALL_Window_30_Test_Called_Seperately.xlsx\",\n",
    "                \"column_gold\" : \"IS_CO_REF\",\n",
    "                \"column_outcome\" : \"Prediction\",\n",
    "                \"threshold\" : 0.5\n",
    "                }\n",
    "\n",
    "model=model_tester(**model_parameter)\n",
    "model.apply_model_to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
